{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fu39oBW0RVn5"
   },
   "source": [
    "# [과제 3] 로지스틱 회귀분석\n",
    "### - sklearn 패키지를 사용해 로지스틱 회귀분석을 진행해주세요.\n",
    "### - 성능지표를 계산하고 이에 대해 해석해주세요.\n",
    "### - 성능 개선을 시도해주세요. (어떠한 성능지표를 기준으로 개선을 시도했는지, 그 이유도 함께 적어주세요.)\n",
    "### - 주석으로 설명 및 근거 자세하게 달아주시면 감사하겠습니다. :)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8rN2SWezRVn_"
   },
   "source": [
    "# 1. Data "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Y7SYKNvQRVn_"
   },
   "source": [
    "출처 : https://www.kaggle.com/mlg-ulb/creditcardfraud\n",
    "\n",
    "\n",
    "* V1 ~ V28 : 비식별화 된 개인정보 \n",
    "* **Class** : Target 변수  \n",
    "  - 1 : fraudulent transactions (사기)\n",
    "  - 0 : otherwise "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "id": "Uvjw2fTCRVoA"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "warnings.filterwarnings(action='ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "id": "znQit70ZRVoA"
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"assignment3_creditcard.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 220
    },
    "id": "v98OeXW5RVoB",
    "outputId": "42afeddc-07e6-4224-95ee-08b455f72475"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>V1</th>\n",
       "      <th>V2</th>\n",
       "      <th>V3</th>\n",
       "      <th>V4</th>\n",
       "      <th>V5</th>\n",
       "      <th>V6</th>\n",
       "      <th>V7</th>\n",
       "      <th>V8</th>\n",
       "      <th>V9</th>\n",
       "      <th>V10</th>\n",
       "      <th>...</th>\n",
       "      <th>V20</th>\n",
       "      <th>V21</th>\n",
       "      <th>V22</th>\n",
       "      <th>V23</th>\n",
       "      <th>V24</th>\n",
       "      <th>V25</th>\n",
       "      <th>V26</th>\n",
       "      <th>V27</th>\n",
       "      <th>V28</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1.848212</td>\n",
       "      <td>2.384900</td>\n",
       "      <td>0.379573</td>\n",
       "      <td>1.048381</td>\n",
       "      <td>-0.845070</td>\n",
       "      <td>2.537837</td>\n",
       "      <td>-4.542983</td>\n",
       "      <td>-10.201458</td>\n",
       "      <td>-1.504967</td>\n",
       "      <td>-2.234167</td>\n",
       "      <td>...</td>\n",
       "      <td>2.585817</td>\n",
       "      <td>-5.291690</td>\n",
       "      <td>0.859364</td>\n",
       "      <td>0.423231</td>\n",
       "      <td>-0.506985</td>\n",
       "      <td>1.020052</td>\n",
       "      <td>-0.627751</td>\n",
       "      <td>-0.017753</td>\n",
       "      <td>0.280982</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2.071805</td>\n",
       "      <td>-0.477943</td>\n",
       "      <td>-1.444444</td>\n",
       "      <td>-0.548657</td>\n",
       "      <td>0.010036</td>\n",
       "      <td>-0.582242</td>\n",
       "      <td>-0.042878</td>\n",
       "      <td>-0.247160</td>\n",
       "      <td>1.171923</td>\n",
       "      <td>-0.342382</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.077306</td>\n",
       "      <td>0.042858</td>\n",
       "      <td>0.390125</td>\n",
       "      <td>0.041569</td>\n",
       "      <td>0.598427</td>\n",
       "      <td>0.098803</td>\n",
       "      <td>0.979686</td>\n",
       "      <td>-0.093244</td>\n",
       "      <td>-0.065615</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-2.985294</td>\n",
       "      <td>-2.747472</td>\n",
       "      <td>1.194068</td>\n",
       "      <td>-0.003036</td>\n",
       "      <td>-1.151041</td>\n",
       "      <td>-0.263559</td>\n",
       "      <td>0.553500</td>\n",
       "      <td>0.635600</td>\n",
       "      <td>0.438545</td>\n",
       "      <td>-1.806488</td>\n",
       "      <td>...</td>\n",
       "      <td>1.345776</td>\n",
       "      <td>0.373760</td>\n",
       "      <td>-0.385777</td>\n",
       "      <td>1.197596</td>\n",
       "      <td>0.407229</td>\n",
       "      <td>0.008013</td>\n",
       "      <td>0.762362</td>\n",
       "      <td>-0.299024</td>\n",
       "      <td>-0.303929</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 29 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         V1        V2        V3        V4        V5        V6        V7  \\\n",
       "0 -1.848212  2.384900  0.379573  1.048381 -0.845070  2.537837 -4.542983   \n",
       "1  2.071805 -0.477943 -1.444444 -0.548657  0.010036 -0.582242 -0.042878   \n",
       "2 -2.985294 -2.747472  1.194068 -0.003036 -1.151041 -0.263559  0.553500   \n",
       "\n",
       "          V8        V9       V10  ...       V20       V21       V22       V23  \\\n",
       "0 -10.201458 -1.504967 -2.234167  ...  2.585817 -5.291690  0.859364  0.423231   \n",
       "1  -0.247160  1.171923 -0.342382  ... -0.077306  0.042858  0.390125  0.041569   \n",
       "2   0.635600  0.438545 -1.806488  ...  1.345776  0.373760 -0.385777  1.197596   \n",
       "\n",
       "        V24       V25       V26       V27       V28  Class  \n",
       "0 -0.506985  1.020052 -0.627751 -0.017753  0.280982      0  \n",
       "1  0.598427  0.098803  0.979686 -0.093244 -0.065615      0  \n",
       "2  0.407229  0.008013  0.762362 -0.299024 -0.303929      0  \n",
       "\n",
       "[3 rows x 29 columns]"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# data를 직접확인합니다.\n",
    "data.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 28678 entries, 0 to 28677\n",
      "Data columns (total 29 columns):\n",
      " #   Column  Non-Null Count  Dtype  \n",
      "---  ------  --------------  -----  \n",
      " 0   V1      28678 non-null  float64\n",
      " 1   V2      28678 non-null  float64\n",
      " 2   V3      28678 non-null  float64\n",
      " 3   V4      28678 non-null  float64\n",
      " 4   V5      28678 non-null  float64\n",
      " 5   V6      28678 non-null  float64\n",
      " 6   V7      28678 non-null  float64\n",
      " 7   V8      28678 non-null  float64\n",
      " 8   V9      28678 non-null  float64\n",
      " 9   V10     28678 non-null  float64\n",
      " 10  V11     28678 non-null  float64\n",
      " 11  V12     28678 non-null  float64\n",
      " 12  V13     28678 non-null  float64\n",
      " 13  V14     28678 non-null  float64\n",
      " 14  V15     28678 non-null  float64\n",
      " 15  V16     28678 non-null  float64\n",
      " 16  V17     28678 non-null  float64\n",
      " 17  V18     28678 non-null  float64\n",
      " 18  V19     28678 non-null  float64\n",
      " 19  V20     28678 non-null  float64\n",
      " 20  V21     28678 non-null  float64\n",
      " 21  V22     28678 non-null  float64\n",
      " 22  V23     28678 non-null  float64\n",
      " 23  V24     28678 non-null  float64\n",
      " 24  V25     28678 non-null  float64\n",
      " 25  V26     28678 non-null  float64\n",
      " 26  V27     28678 non-null  float64\n",
      " 27  V28     28678 non-null  float64\n",
      " 28  Class   28678 non-null  int64  \n",
      "dtypes: float64(28), int64(1)\n",
      "memory usage: 6.3 MB\n"
     ]
    }
   ],
   "source": [
    "# 변수별 datatype및 변수명을 확인합니다.\n",
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>V1</th>\n",
       "      <th>V2</th>\n",
       "      <th>V3</th>\n",
       "      <th>V4</th>\n",
       "      <th>V5</th>\n",
       "      <th>V6</th>\n",
       "      <th>V7</th>\n",
       "      <th>V8</th>\n",
       "      <th>V9</th>\n",
       "      <th>V10</th>\n",
       "      <th>...</th>\n",
       "      <th>V20</th>\n",
       "      <th>V21</th>\n",
       "      <th>V22</th>\n",
       "      <th>V23</th>\n",
       "      <th>V24</th>\n",
       "      <th>V25</th>\n",
       "      <th>V26</th>\n",
       "      <th>V27</th>\n",
       "      <th>V28</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>28678.000000</td>\n",
       "      <td>28678.000000</td>\n",
       "      <td>28678.000000</td>\n",
       "      <td>28678.000000</td>\n",
       "      <td>28678.000000</td>\n",
       "      <td>28678.000000</td>\n",
       "      <td>28678.000000</td>\n",
       "      <td>28678.000000</td>\n",
       "      <td>28678.000000</td>\n",
       "      <td>28678.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>28678.000000</td>\n",
       "      <td>28678.000000</td>\n",
       "      <td>28678.000000</td>\n",
       "      <td>28678.000000</td>\n",
       "      <td>28678.000000</td>\n",
       "      <td>28678.000000</td>\n",
       "      <td>28678.000000</td>\n",
       "      <td>28678.000000</td>\n",
       "      <td>28678.000000</td>\n",
       "      <td>28678.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>28432.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>-0.015438</td>\n",
       "      <td>0.053653</td>\n",
       "      <td>-0.046031</td>\n",
       "      <td>0.037348</td>\n",
       "      <td>-0.033724</td>\n",
       "      <td>-0.003299</td>\n",
       "      <td>-0.051054</td>\n",
       "      <td>0.006064</td>\n",
       "      <td>-0.018530</td>\n",
       "      <td>-0.041149</td>\n",
       "      <td>...</td>\n",
       "      <td>0.002633</td>\n",
       "      <td>0.010289</td>\n",
       "      <td>-0.000656</td>\n",
       "      <td>-0.004800</td>\n",
       "      <td>-0.000897</td>\n",
       "      <td>-0.001989</td>\n",
       "      <td>-0.000765</td>\n",
       "      <td>0.000948</td>\n",
       "      <td>0.001535</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>2.031529</td>\n",
       "      <td>1.616186</td>\n",
       "      <td>1.758169</td>\n",
       "      <td>1.482109</td>\n",
       "      <td>1.486998</td>\n",
       "      <td>1.339259</td>\n",
       "      <td>1.454827</td>\n",
       "      <td>1.364342</td>\n",
       "      <td>1.134065</td>\n",
       "      <td>1.252593</td>\n",
       "      <td>...</td>\n",
       "      <td>0.720307</td>\n",
       "      <td>0.847152</td>\n",
       "      <td>0.739469</td>\n",
       "      <td>0.593663</td>\n",
       "      <td>0.603349</td>\n",
       "      <td>0.517968</td>\n",
       "      <td>0.483852</td>\n",
       "      <td>0.397075</td>\n",
       "      <td>0.296736</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>-30.552380</td>\n",
       "      <td>-42.172688</td>\n",
       "      <td>-31.103685</td>\n",
       "      <td>-5.560118</td>\n",
       "      <td>-42.147898</td>\n",
       "      <td>-21.929312</td>\n",
       "      <td>-41.506796</td>\n",
       "      <td>-39.267378</td>\n",
       "      <td>-13.434066</td>\n",
       "      <td>-24.403185</td>\n",
       "      <td>...</td>\n",
       "      <td>-21.387122</td>\n",
       "      <td>-21.453736</td>\n",
       "      <td>-8.887017</td>\n",
       "      <td>-36.666000</td>\n",
       "      <td>-2.718024</td>\n",
       "      <td>-6.712624</td>\n",
       "      <td>-2.241620</td>\n",
       "      <td>-7.418878</td>\n",
       "      <td>-9.617915</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>-0.916927</td>\n",
       "      <td>-0.575381</td>\n",
       "      <td>-0.899872</td>\n",
       "      <td>-0.843321</td>\n",
       "      <td>-0.714901</td>\n",
       "      <td>-0.763757</td>\n",
       "      <td>-0.568146</td>\n",
       "      <td>-0.206103</td>\n",
       "      <td>-0.661909</td>\n",
       "      <td>-0.543450</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.209678</td>\n",
       "      <td>-0.225520</td>\n",
       "      <td>-0.539244</td>\n",
       "      <td>-0.160583</td>\n",
       "      <td>-0.356047</td>\n",
       "      <td>-0.318619</td>\n",
       "      <td>-0.327343</td>\n",
       "      <td>-0.070558</td>\n",
       "      <td>-0.052189</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.020050</td>\n",
       "      <td>0.075358</td>\n",
       "      <td>0.180610</td>\n",
       "      <td>-0.008844</td>\n",
       "      <td>-0.060040</td>\n",
       "      <td>-0.271363</td>\n",
       "      <td>0.036107</td>\n",
       "      <td>0.022463</td>\n",
       "      <td>-0.055095</td>\n",
       "      <td>-0.097390</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.062792</td>\n",
       "      <td>-0.028778</td>\n",
       "      <td>0.007302</td>\n",
       "      <td>-0.011199</td>\n",
       "      <td>0.040006</td>\n",
       "      <td>0.019770</td>\n",
       "      <td>-0.056260</td>\n",
       "      <td>0.002049</td>\n",
       "      <td>0.011075</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1.317461</td>\n",
       "      <td>0.806957</td>\n",
       "      <td>1.029928</td>\n",
       "      <td>0.771958</td>\n",
       "      <td>0.613328</td>\n",
       "      <td>0.397269</td>\n",
       "      <td>0.559409</td>\n",
       "      <td>0.329606</td>\n",
       "      <td>0.605704</td>\n",
       "      <td>0.460681</td>\n",
       "      <td>...</td>\n",
       "      <td>0.131199</td>\n",
       "      <td>0.184312</td>\n",
       "      <td>0.526358</td>\n",
       "      <td>0.146835</td>\n",
       "      <td>0.437146</td>\n",
       "      <td>0.352717</td>\n",
       "      <td>0.240713</td>\n",
       "      <td>0.091637</td>\n",
       "      <td>0.078911</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>2.399484</td>\n",
       "      <td>21.467203</td>\n",
       "      <td>4.069865</td>\n",
       "      <td>11.927512</td>\n",
       "      <td>32.911462</td>\n",
       "      <td>22.529298</td>\n",
       "      <td>36.677268</td>\n",
       "      <td>20.007208</td>\n",
       "      <td>8.113152</td>\n",
       "      <td>15.236028</td>\n",
       "      <td>...</td>\n",
       "      <td>26.237391</td>\n",
       "      <td>27.202839</td>\n",
       "      <td>8.361985</td>\n",
       "      <td>9.637187</td>\n",
       "      <td>3.948061</td>\n",
       "      <td>2.510401</td>\n",
       "      <td>3.122747</td>\n",
       "      <td>11.135740</td>\n",
       "      <td>14.929133</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>11 rows × 29 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                  V1            V2            V3            V4            V5  \\\n",
       "count   28678.000000  28678.000000  28678.000000  28678.000000  28678.000000   \n",
       "unique           NaN           NaN           NaN           NaN           NaN   \n",
       "top              NaN           NaN           NaN           NaN           NaN   \n",
       "freq             NaN           NaN           NaN           NaN           NaN   \n",
       "mean       -0.015438      0.053653     -0.046031      0.037348     -0.033724   \n",
       "std         2.031529      1.616186      1.758169      1.482109      1.486998   \n",
       "min       -30.552380    -42.172688    -31.103685     -5.560118    -42.147898   \n",
       "25%        -0.916927     -0.575381     -0.899872     -0.843321     -0.714901   \n",
       "50%         0.020050      0.075358      0.180610     -0.008844     -0.060040   \n",
       "75%         1.317461      0.806957      1.029928      0.771958      0.613328   \n",
       "max         2.399484     21.467203      4.069865     11.927512     32.911462   \n",
       "\n",
       "                  V6            V7            V8            V9           V10  \\\n",
       "count   28678.000000  28678.000000  28678.000000  28678.000000  28678.000000   \n",
       "unique           NaN           NaN           NaN           NaN           NaN   \n",
       "top              NaN           NaN           NaN           NaN           NaN   \n",
       "freq             NaN           NaN           NaN           NaN           NaN   \n",
       "mean       -0.003299     -0.051054      0.006064     -0.018530     -0.041149   \n",
       "std         1.339259      1.454827      1.364342      1.134065      1.252593   \n",
       "min       -21.929312    -41.506796    -39.267378    -13.434066    -24.403185   \n",
       "25%        -0.763757     -0.568146     -0.206103     -0.661909     -0.543450   \n",
       "50%        -0.271363      0.036107      0.022463     -0.055095     -0.097390   \n",
       "75%         0.397269      0.559409      0.329606      0.605704      0.460681   \n",
       "max        22.529298     36.677268     20.007208      8.113152     15.236028   \n",
       "\n",
       "        ...           V20           V21           V22           V23  \\\n",
       "count   ...  28678.000000  28678.000000  28678.000000  28678.000000   \n",
       "unique  ...           NaN           NaN           NaN           NaN   \n",
       "top     ...           NaN           NaN           NaN           NaN   \n",
       "freq    ...           NaN           NaN           NaN           NaN   \n",
       "mean    ...      0.002633      0.010289     -0.000656     -0.004800   \n",
       "std     ...      0.720307      0.847152      0.739469      0.593663   \n",
       "min     ...    -21.387122    -21.453736     -8.887017    -36.666000   \n",
       "25%     ...     -0.209678     -0.225520     -0.539244     -0.160583   \n",
       "50%     ...     -0.062792     -0.028778      0.007302     -0.011199   \n",
       "75%     ...      0.131199      0.184312      0.526358      0.146835   \n",
       "max     ...     26.237391     27.202839      8.361985      9.637187   \n",
       "\n",
       "                 V24           V25           V26           V27           V28  \\\n",
       "count   28678.000000  28678.000000  28678.000000  28678.000000  28678.000000   \n",
       "unique           NaN           NaN           NaN           NaN           NaN   \n",
       "top              NaN           NaN           NaN           NaN           NaN   \n",
       "freq             NaN           NaN           NaN           NaN           NaN   \n",
       "mean       -0.000897     -0.001989     -0.000765      0.000948      0.001535   \n",
       "std         0.603349      0.517968      0.483852      0.397075      0.296736   \n",
       "min        -2.718024     -6.712624     -2.241620     -7.418878     -9.617915   \n",
       "25%        -0.356047     -0.318619     -0.327343     -0.070558     -0.052189   \n",
       "50%         0.040006      0.019770     -0.056260      0.002049      0.011075   \n",
       "75%         0.437146      0.352717      0.240713      0.091637      0.078911   \n",
       "max         3.948061      2.510401      3.122747     11.135740     14.929133   \n",
       "\n",
       "          Class  \n",
       "count   28678.0  \n",
       "unique      2.0  \n",
       "top         0.0  \n",
       "freq    28432.0  \n",
       "mean        NaN  \n",
       "std         NaN  \n",
       "min         NaN  \n",
       "25%         NaN  \n",
       "50%         NaN  \n",
       "75%         NaN  \n",
       "max         NaN  \n",
       "\n",
       "[11 rows x 29 columns]"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 범주형 변수를 처리합니다.\n",
    "data[\"Class\"] = pd.Series(data[\"Class\"], dtype = \"category\")\n",
    "features = data.iloc[:, :-1]\n",
    "target = data.iloc[:, -1]\n",
    "\n",
    "\n",
    "# 각 변수별 기초통계량을 확인합니다.\n",
    "data.describe(include='all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>V1</th>\n",
       "      <th>V2</th>\n",
       "      <th>V3</th>\n",
       "      <th>V4</th>\n",
       "      <th>V5</th>\n",
       "      <th>V6</th>\n",
       "      <th>V7</th>\n",
       "      <th>V8</th>\n",
       "      <th>V9</th>\n",
       "      <th>V10</th>\n",
       "      <th>...</th>\n",
       "      <th>V19</th>\n",
       "      <th>V20</th>\n",
       "      <th>V21</th>\n",
       "      <th>V22</th>\n",
       "      <th>V23</th>\n",
       "      <th>V24</th>\n",
       "      <th>V25</th>\n",
       "      <th>V26</th>\n",
       "      <th>V27</th>\n",
       "      <th>V28</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>28678.000000</td>\n",
       "      <td>28678.000000</td>\n",
       "      <td>28678.000000</td>\n",
       "      <td>28678.000000</td>\n",
       "      <td>28678.000000</td>\n",
       "      <td>28678.000000</td>\n",
       "      <td>28678.000000</td>\n",
       "      <td>28678.000000</td>\n",
       "      <td>28678.000000</td>\n",
       "      <td>28678.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>28678.000000</td>\n",
       "      <td>28678.000000</td>\n",
       "      <td>28678.000000</td>\n",
       "      <td>28678.000000</td>\n",
       "      <td>28678.000000</td>\n",
       "      <td>28678.000000</td>\n",
       "      <td>28678.000000</td>\n",
       "      <td>28678.000000</td>\n",
       "      <td>28678.000000</td>\n",
       "      <td>28678.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.926714</td>\n",
       "      <td>0.663520</td>\n",
       "      <td>0.882983</td>\n",
       "      <td>0.320081</td>\n",
       "      <td>0.561078</td>\n",
       "      <td>0.493178</td>\n",
       "      <td>0.530233</td>\n",
       "      <td>0.662568</td>\n",
       "      <td>0.622611</td>\n",
       "      <td>0.614594</td>\n",
       "      <td>...</td>\n",
       "      <td>0.469792</td>\n",
       "      <td>0.449133</td>\n",
       "      <td>0.441133</td>\n",
       "      <td>0.515181</td>\n",
       "      <td>0.791764</td>\n",
       "      <td>0.407605</td>\n",
       "      <td>0.727596</td>\n",
       "      <td>0.417730</td>\n",
       "      <td>0.399891</td>\n",
       "      <td>0.391878</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.061651</td>\n",
       "      <td>0.025396</td>\n",
       "      <td>0.049986</td>\n",
       "      <td>0.084752</td>\n",
       "      <td>0.019811</td>\n",
       "      <td>0.030124</td>\n",
       "      <td>0.018608</td>\n",
       "      <td>0.023017</td>\n",
       "      <td>0.052632</td>\n",
       "      <td>0.031600</td>\n",
       "      <td>...</td>\n",
       "      <td>0.084024</td>\n",
       "      <td>0.015125</td>\n",
       "      <td>0.017411</td>\n",
       "      <td>0.042870</td>\n",
       "      <td>0.012821</td>\n",
       "      <td>0.090510</td>\n",
       "      <td>0.056160</td>\n",
       "      <td>0.090197</td>\n",
       "      <td>0.021400</td>\n",
       "      <td>0.012088</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.899356</td>\n",
       "      <td>0.653636</td>\n",
       "      <td>0.858708</td>\n",
       "      <td>0.269722</td>\n",
       "      <td>0.552003</td>\n",
       "      <td>0.476073</td>\n",
       "      <td>0.523619</td>\n",
       "      <td>0.658989</td>\n",
       "      <td>0.592752</td>\n",
       "      <td>0.601923</td>\n",
       "      <td>...</td>\n",
       "      <td>0.422444</td>\n",
       "      <td>0.444675</td>\n",
       "      <td>0.436287</td>\n",
       "      <td>0.483957</td>\n",
       "      <td>0.788400</td>\n",
       "      <td>0.354327</td>\n",
       "      <td>0.693265</td>\n",
       "      <td>0.356851</td>\n",
       "      <td>0.396037</td>\n",
       "      <td>0.389689</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.927791</td>\n",
       "      <td>0.663861</td>\n",
       "      <td>0.889427</td>\n",
       "      <td>0.317440</td>\n",
       "      <td>0.560728</td>\n",
       "      <td>0.487149</td>\n",
       "      <td>0.531347</td>\n",
       "      <td>0.662845</td>\n",
       "      <td>0.620914</td>\n",
       "      <td>0.613176</td>\n",
       "      <td>...</td>\n",
       "      <td>0.469277</td>\n",
       "      <td>0.447760</td>\n",
       "      <td>0.440330</td>\n",
       "      <td>0.515643</td>\n",
       "      <td>0.791626</td>\n",
       "      <td>0.413741</td>\n",
       "      <td>0.729955</td>\n",
       "      <td>0.407385</td>\n",
       "      <td>0.399950</td>\n",
       "      <td>0.392267</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.967164</td>\n",
       "      <td>0.675357</td>\n",
       "      <td>0.913573</td>\n",
       "      <td>0.362089</td>\n",
       "      <td>0.569699</td>\n",
       "      <td>0.502188</td>\n",
       "      <td>0.538041</td>\n",
       "      <td>0.668026</td>\n",
       "      <td>0.651582</td>\n",
       "      <td>0.627254</td>\n",
       "      <td>...</td>\n",
       "      <td>0.516566</td>\n",
       "      <td>0.451833</td>\n",
       "      <td>0.444710</td>\n",
       "      <td>0.545734</td>\n",
       "      <td>0.795039</td>\n",
       "      <td>0.473317</td>\n",
       "      <td>0.766055</td>\n",
       "      <td>0.462745</td>\n",
       "      <td>0.404779</td>\n",
       "      <td>0.395030</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 28 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 V1            V2            V3            V4            V5  \\\n",
       "count  28678.000000  28678.000000  28678.000000  28678.000000  28678.000000   \n",
       "mean       0.926714      0.663520      0.882983      0.320081      0.561078   \n",
       "std        0.061651      0.025396      0.049986      0.084752      0.019811   \n",
       "min        0.000000      0.000000      0.000000      0.000000      0.000000   \n",
       "25%        0.899356      0.653636      0.858708      0.269722      0.552003   \n",
       "50%        0.927791      0.663861      0.889427      0.317440      0.560728   \n",
       "75%        0.967164      0.675357      0.913573      0.362089      0.569699   \n",
       "max        1.000000      1.000000      1.000000      1.000000      1.000000   \n",
       "\n",
       "                 V6            V7            V8            V9           V10  \\\n",
       "count  28678.000000  28678.000000  28678.000000  28678.000000  28678.000000   \n",
       "mean       0.493178      0.530233      0.662568      0.622611      0.614594   \n",
       "std        0.030124      0.018608      0.023017      0.052632      0.031600   \n",
       "min        0.000000      0.000000      0.000000      0.000000      0.000000   \n",
       "25%        0.476073      0.523619      0.658989      0.592752      0.601923   \n",
       "50%        0.487149      0.531347      0.662845      0.620914      0.613176   \n",
       "75%        0.502188      0.538041      0.668026      0.651582      0.627254   \n",
       "max        1.000000      1.000000      1.000000      1.000000      1.000000   \n",
       "\n",
       "       ...           V19           V20           V21           V22  \\\n",
       "count  ...  28678.000000  28678.000000  28678.000000  28678.000000   \n",
       "mean   ...      0.469792      0.449133      0.441133      0.515181   \n",
       "std    ...      0.084024      0.015125      0.017411      0.042870   \n",
       "min    ...      0.000000      0.000000      0.000000      0.000000   \n",
       "25%    ...      0.422444      0.444675      0.436287      0.483957   \n",
       "50%    ...      0.469277      0.447760      0.440330      0.515643   \n",
       "75%    ...      0.516566      0.451833      0.444710      0.545734   \n",
       "max    ...      1.000000      1.000000      1.000000      1.000000   \n",
       "\n",
       "                V23           V24           V25           V26           V27  \\\n",
       "count  28678.000000  28678.000000  28678.000000  28678.000000  28678.000000   \n",
       "mean       0.791764      0.407605      0.727596      0.417730      0.399891   \n",
       "std        0.012821      0.090510      0.056160      0.090197      0.021400   \n",
       "min        0.000000      0.000000      0.000000      0.000000      0.000000   \n",
       "25%        0.788400      0.354327      0.693265      0.356851      0.396037   \n",
       "50%        0.791626      0.413741      0.729955      0.407385      0.399950   \n",
       "75%        0.795039      0.473317      0.766055      0.462745      0.404779   \n",
       "max        1.000000      1.000000      1.000000      1.000000      1.000000   \n",
       "\n",
       "                V28  \n",
       "count  28678.000000  \n",
       "mean       0.391878  \n",
       "std        0.012088  \n",
       "min        0.000000  \n",
       "25%        0.389689  \n",
       "50%        0.392267  \n",
       "75%        0.395030  \n",
       "max        1.000000  \n",
       "\n",
       "[8 rows x 28 columns]"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Scaling\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "s_features = pd.DataFrame(scaler.fit_transform(features), columns= features.columns)\n",
    "\n",
    "# scaling은 설명 변수에만 진행합니다.\n",
    "s_features.describe(include='all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    28432\n",
       "1      246\n",
       "Name: Class, dtype: int64"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# target변수의 분포를 확인합니다.\n",
    "y.value_counts()\n",
    "\n",
    "# 상당히 편향 되어 있는 것을 확인할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import *\n",
    "\n",
    "# train과 test로 나누어 줍니다.\n",
    "X_train, X_test, y_train, y_test = train_test_split(s_features, target, test_size=0.2, random_state = 11, stratify = target)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(22942, 28) (5736, 28) (22942,) (5736,)\n"
     ]
    }
   ],
   "source": [
    "# 나눈 data의 shape을 확인합니다.\n",
    "print(X_train.shape, X_test.shape, y_train.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression()"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Logistic Regression을 적합합니다.\n",
    "model = LogisticRegression()\n",
    "model.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, ..., 0, 0, 0], dtype=int64)"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test dataset의 class를 예측합니다.\n",
    "y_pred = model.predict(X_test) \n",
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.9976856 , 0.0023144 ],\n",
       "       [0.99399716, 0.00600284],\n",
       "       [0.99858368, 0.00141632],\n",
       "       ...,\n",
       "       [0.99853482, 0.00146518],\n",
       "       [0.99868098, 0.00131902],\n",
       "       [0.99828671, 0.00171329]])"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# [0으로 분류될 확률, 1로 분류될 확률]\n",
    "model.predict_proba(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. 성능지표"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[5685,   19],\n",
       "       [   2,   30]], dtype=int64)"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(y_pred, y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy :  0.9963389121338913\n",
      "precision :  0.9375\n",
      "Recall :  0.6122448979591837\n",
      "f1 :  0.7407407407407408\n"
     ]
    }
   ],
   "source": [
    "tn, fn, fp, tp = confusion_matrix(y_pred, y_test).ravel()\n",
    "p = fn + tp\n",
    "n = tn + fp\n",
    "\n",
    "accuracy = (tp + tn) / (p + n)\n",
    "print(\"Accuracy : \", accuracy) \n",
    "\n",
    "precision = tp/ (tp + fp)\n",
    "print(\"precision : \", precision)\n",
    "\n",
    "recall = (tp) / p\n",
    "print(\"Recall : \", recall)\n",
    "\n",
    "f1 = f1_score(y_pred, y_test)\n",
    "print(\"f1 : \", f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "위에서 data가 상당히 편향되어 있는 것을 확인 하였습니다.  \n",
    "0이 너무 많습니다.  \n",
    "따라서 ACC가 항상 높게 나올수 밖에 없습니다.  \n",
    "예를들어 위 모델이 항상 0만을 output하는 모델이라면 확실하게 성능이 좋지 않은 모델이지만  \n",
    "ACC는 (28432 / (28432+246) ) = 0.9914219959550875의 값을 가집니다.  \n",
    "따라서 이 데이터에는 ACC의 값은 좋은 지표가 되지 못합니다.  \n",
    "이를 보완하고자  Precision과 Recall의 조화 평균인 F1 score를 보아야 합니다.  \n",
    "Precision과 Recall는 trade - off관계 이므로 cutoff를 조정하면서 직접 확인하는 것이 성능개선에 도움이 됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "column_name = data.keys()\n",
    "beta = np.concatenate([model.intercept_,model.coef_.reshape(-1)]).round(2)\n",
    "odds = np.exp(beta).round(2)\n",
    "\n",
    "beta_analysis = pd.DataFrame(np.c_[beta,odds],index=column_name,columns=[\"beta\",\"exp(beta)\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current cutoff is : 0.01\n",
      "Accuracy :  0.9879707112970711\n",
      "precision :  0.41228070175438597\n",
      "Recall :  0.9591836734693877\n",
      "f1 :  0.5766871165644172\n",
      "------------------------------\n",
      "current cutoff is : 0.06157894736842105\n",
      "Accuracy :  0.9977336122733612\n",
      "precision :  0.875\n",
      "Recall :  0.8571428571428571\n",
      "f1 :  0.8659793814432989\n",
      "------------------------------\n",
      "current cutoff is : 0.1131578947368421\n",
      "Accuracy :  0.9980822873082287\n",
      "precision :  0.9318181818181818\n",
      "Recall :  0.8367346938775511\n",
      "f1 :  0.8817204301075268\n",
      "------------------------------\n",
      "current cutoff is : 0.16473684210526315\n",
      "Accuracy :  0.9980822873082287\n",
      "precision :  0.9523809523809523\n",
      "Recall :  0.8163265306122449\n",
      "f1 :  0.8791208791208791\n",
      "------------------------------\n",
      "current cutoff is : 0.2163157894736842\n",
      "Accuracy :  0.997907949790795\n",
      "precision :  0.9512195121951219\n",
      "Recall :  0.7959183673469388\n",
      "f1 :  0.8666666666666666\n",
      "------------------------------\n",
      "current cutoff is : 0.26789473684210524\n",
      "Accuracy :  0.99721059972106\n",
      "precision :  0.9459459459459459\n",
      "Recall :  0.7142857142857143\n",
      "f1 :  0.813953488372093\n",
      "------------------------------\n",
      "current cutoff is : 0.3194736842105263\n",
      "Accuracy :  0.9966875871687587\n",
      "precision :  0.9411764705882353\n",
      "Recall :  0.6530612244897959\n",
      "f1 :  0.7710843373493975\n",
      "------------------------------\n",
      "current cutoff is : 0.37105263157894736\n",
      "Accuracy :  0.9963389121338913\n",
      "precision :  0.9375\n",
      "Recall :  0.6122448979591837\n",
      "f1 :  0.7407407407407408\n",
      "------------------------------\n",
      "current cutoff is : 0.4226315789473684\n",
      "Accuracy :  0.9963389121338913\n",
      "precision :  0.9375\n",
      "Recall :  0.6122448979591837\n",
      "f1 :  0.7407407407407408\n",
      "------------------------------\n",
      "current cutoff is : 0.47421052631578947\n",
      "Accuracy :  0.9963389121338913\n",
      "precision :  0.9375\n",
      "Recall :  0.6122448979591837\n",
      "f1 :  0.7407407407407408\n",
      "------------------------------\n",
      "current cutoff is : 0.5257894736842105\n",
      "Accuracy :  0.9963389121338913\n",
      "precision :  0.9375\n",
      "Recall :  0.6122448979591837\n",
      "f1 :  0.7407407407407408\n",
      "------------------------------\n",
      "current cutoff is : 0.5773684210526315\n",
      "Accuracy :  0.9963389121338913\n",
      "precision :  0.9375\n",
      "Recall :  0.6122448979591837\n",
      "f1 :  0.7407407407407408\n",
      "------------------------------\n",
      "current cutoff is : 0.6289473684210526\n",
      "Accuracy :  0.9963389121338913\n",
      "precision :  0.9375\n",
      "Recall :  0.6122448979591837\n",
      "f1 :  0.7407407407407408\n",
      "------------------------------\n",
      "current cutoff is : 0.6805263157894736\n",
      "Accuracy :  0.9961645746164575\n",
      "precision :  0.9354838709677419\n",
      "Recall :  0.5918367346938775\n",
      "f1 :  0.7249999999999999\n",
      "------------------------------\n",
      "current cutoff is : 0.7321052631578947\n",
      "Accuracy :  0.9959902370990237\n",
      "precision :  0.9333333333333333\n",
      "Recall :  0.5714285714285714\n",
      "f1 :  0.7088607594936709\n",
      "------------------------------\n",
      "current cutoff is : 0.7836842105263158\n",
      "Accuracy :  0.99581589958159\n",
      "precision :  0.9310344827586207\n",
      "Recall :  0.5510204081632653\n",
      "f1 :  0.6923076923076924\n",
      "------------------------------\n",
      "current cutoff is : 0.8352631578947368\n",
      "Accuracy :  0.9956415620641562\n",
      "precision :  0.9285714285714286\n",
      "Recall :  0.5306122448979592\n",
      "f1 :  0.6753246753246754\n",
      "------------------------------\n",
      "current cutoff is : 0.8868421052631579\n",
      "Accuracy :  0.9956415620641562\n",
      "precision :  0.9285714285714286\n",
      "Recall :  0.5306122448979592\n",
      "f1 :  0.6753246753246754\n",
      "------------------------------\n",
      "current cutoff is : 0.9384210526315789\n",
      "Accuracy :  0.9956415620641562\n",
      "precision :  0.9285714285714286\n",
      "Recall :  0.5306122448979592\n",
      "f1 :  0.6753246753246754\n",
      "------------------------------\n",
      "current cutoff is : 0.99\n",
      "Accuracy :  0.9944211994421199\n",
      "precision :  0.9473684210526315\n",
      "Recall :  0.3673469387755102\n",
      "f1 :  0.5294117647058824\n",
      "------------------------------\n"
     ]
    }
   ],
   "source": [
    "Xbeta=np.matmul(np.c_[np.ones(X_test.shape[0]),X_test],beta.reshape(-1,1))\n",
    "\n",
    "#P(Y=1) 계산\n",
    "P_1=1/(1+np.exp(-Xbeta))\n",
    "\n",
    "f1_list = list()\n",
    "\n",
    "Cut_off=np.linspace(0.01,0.99,20) #cut off 값 만들기\n",
    "for cutoff in Cut_off:\n",
    "  y_pred=np.where(P_1.reshape(-1)>=cutoff,1,0)\n",
    "  \n",
    "  tn, fn, fp, tp = confusion_matrix(y_pred, y_test).ravel()\n",
    "  p = fn + tp\n",
    "  n = tn + fp\n",
    "\n",
    "  print(\"current cutoff is :\", str(cutoff))\n",
    "  \n",
    "  accuracy = (tp + tn) / (p + n)\n",
    "  print(\"Accuracy : \", accuracy) \n",
    "\n",
    "  precision = tp/ (tp + fp)\n",
    "  print(\"precision : \", precision)\n",
    "\n",
    "  recall = (tp) / p\n",
    "  print(\"Recall : \", recall)\n",
    "\n",
    "  f1 = f1_score(y_pred, y_test)\n",
    "  print(\"f1 : \", f1)\n",
    "  print('-'*30)\n",
    "  f1_list += [f1]\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x1fd03100048>]"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(np.linspace(0.01,0.99,20),f1_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current cutoff is : 0.1131578947368421\n",
      "Accuracy :  0.9980822873082287\n",
      "precision :  0.9318181818181818\n",
      "Recall :  0.8367346938775511\n",
      "f1 :  0.8817204301075268\n",
      "------------------------------\n"
     ]
    }
   ],
   "source": [
    "\n",
    "Xbeta=np.matmul(np.c_[np.ones(X_test.shape[0]),X_test],beta.reshape(-1,1))\n",
    "\n",
    "#P(Y=1) 계산\n",
    "P_1=1/(1+np.exp(-Xbeta))\n",
    "\n",
    "f1_list = list()\n",
    "\n",
    "Cut_off= [0.1131578947368421] #cut off 값 만들기\n",
    "for cutoff in Cut_off:\n",
    "  y_pred=np.where(P_1.reshape(-1)>=cutoff,1,0)\n",
    "  \n",
    "  tn, fn, fp, tp = confusion_matrix(y_pred, y_test).ravel()\n",
    "  p = fn + tp\n",
    "  n = tn + fp\n",
    "\n",
    "  print(\"current cutoff is :\", str(cutoff))\n",
    "  \n",
    "  accuracy = (tp + tn) / (p + n)\n",
    "  print(\"Accuracy : \", accuracy) \n",
    "\n",
    "  precision = tp/ (tp + fp)\n",
    "  print(\"precision : \", precision)\n",
    "\n",
    "  recall = (tp) / p\n",
    "  print(\"Recall : \", recall)\n",
    "\n",
    "  f1 = f1_score(y_pred, y_test)\n",
    "  print(\"f1 : \", f1)\n",
    "  print('-'*30)\n",
    "  f1_list += [f1]\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current cutoff is : 0.5\n",
      "Accuracy :  0.9963389121338913\n",
      "precision :  0.9375\n",
      "Recall :  0.6122448979591837\n",
      "f1 :  0.7407407407407408\n",
      "------------------------------\n",
      "current cutoff is : 0.1131578947368421\n",
      "Accuracy :  0.9980822873082287\n",
      "precision :  0.9318181818181818\n",
      "Recall :  0.8367346938775511\n",
      "f1 :  0.8817204301075268\n",
      "------------------------------\n"
     ]
    }
   ],
   "source": [
    "# cutoff 비교\n",
    "\n",
    "Cut_off= [0.5] #cut off 값 만들기\n",
    "for cutoff in Cut_off:\n",
    "  y_pred=np.where(P_1.reshape(-1)>=cutoff,1,0)\n",
    "  \n",
    "  tn, fn, fp, tp = confusion_matrix(y_pred, y_test).ravel()\n",
    "  p = fn + tp\n",
    "  n = tn + fp\n",
    "\n",
    "  print(\"current cutoff is :\", str(cutoff))\n",
    "  \n",
    "  accuracy = (tp + tn) / (p + n)\n",
    "  print(\"Accuracy : \", accuracy) \n",
    "\n",
    "  precision = tp/ (tp + fp)\n",
    "  print(\"precision : \", precision)\n",
    "\n",
    "  recall = (tp) / p\n",
    "  print(\"Recall : \", recall)\n",
    "\n",
    "  f1 = f1_score(y_pred, y_test)\n",
    "  print(\"f1 : \", f1)\n",
    "  print('-'*30)\n",
    "  f1_list += [f1]\n",
    "  \n",
    "Cut_off= [0.1131578947368421] #cut off 값 만들기\n",
    "for cutoff in Cut_off:\n",
    "  y_pred=np.where(P_1.reshape(-1)>=cutoff,1,0)\n",
    "  \n",
    "  tn, fn, fp, tp = confusion_matrix(y_pred, y_test).ravel()\n",
    "  p = fn + tp\n",
    "  n = tn + fp\n",
    "\n",
    "  print(\"current cutoff is :\", str(cutoff))\n",
    "  \n",
    "  accuracy = (tp + tn) / (p + n)\n",
    "  print(\"Accuracy : \", accuracy) \n",
    "\n",
    "  precision = tp/ (tp + fp)\n",
    "  print(\"precision : \", precision)\n",
    "\n",
    "  recall = (tp) / p\n",
    "  print(\"Recall : \", recall)\n",
    "\n",
    "  f1 = f1_score(y_pred, y_test)\n",
    "  print(\"f1 : \", f1)\n",
    "  print('-'*30)\n",
    "  f1_list += [f1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "순서대로 cutoff 변경 전과 변경 후 입니다.  \n",
    "성능이 개선 된 것을 확인할 수 있습니다."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Regression_과제3",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
